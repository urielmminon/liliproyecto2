{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto Final - Métodos de Gran Escala\n",
    "\n",
    "\n",
    "# Generar un Pipeline en Spark en cluster AWS \n",
    "\n",
    "\n",
    "## Equipo 1:\n",
    "\n",
    "    Itzel Muñoz: 122803\n",
    "    Uriel Miranda: 177508\n",
    "    Luis Puente: 103108\n",
    "    Juan Martínez Parente: 124458"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import findspark\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "findspark.init(os.environ['SPARK_HOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pyspark \n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *  \n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, SQLTransformer, Normalizer, VectorAssembler, VectorIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.regression import RandomForestRegressor,LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sesión de spark en cluster de AWS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creamos la sesión de spark\n",
    "spark = SparkSession\\\n",
    ".builder\\\n",
    ".appName(\"spark_Parte2\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.71:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark_Parte2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc852c59128>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>  Insertar imagen del cluster </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Declaración de bases de datas y tipo de variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección se declaran las bases de datos y el tipo de variables que se utilizarán más adelante para correr los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "profecoSchema = StructType([StructField(\"producto\", StringType(), True), \\\n",
    "                     StructField(\"presentacion\", StringType(), True), \\\n",
    "                     StructField(\"marca\", StringType(), True), \\\n",
    "                     StructField(\"categoria\", StringType(), True), \\\n",
    "                     StructField(\"catalogo\", StringType(), True), \\\n",
    "                     StructField(\"precio\", DoubleType(), True), \\\n",
    "                     StructField(\"fecharegistro\", TimestampType(), True), \\\n",
    "                     StructField(\"cadenacomercial\", StringType(), True), \\\n",
    "                     StructField(\"giro\", StringType(), True), \\\n",
    "                     StructField(\"nombrecomercial\", StringType(), True), \\\n",
    "                     StructField(\"direccion\", StringType(), True), \\\n",
    "                     StructField(\"estado\", StringType(), True), \\\n",
    "                     StructField(\"municipio\", StringType(), True), \\\n",
    "                     StructField(\"latitud\", DoubleType(), True), \\\n",
    "                     StructField(\"longitud\", DoubleType(), True)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Path does not exist: file:/home/luis/Documents/Maestría en ciencia de datos/Métodos de gran escala/Proyecto2/liliproyecto2/profeco_final_bash.csv;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/spark/spark-2.3.2-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-2.3.2-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o47.load.\n: org.apache.spark.sql.AnalysisException: Path does not exist: file:/home/luis/Documents/Maestría en ciencia de datos/Métodos de gran escala/Proyecto2/liliproyecto2/profeco_final_bash.csv;\n\tat org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:719)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:390)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:390)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:344)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:389)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-53bf638c288f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprofecoDf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delimiter\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"|\"\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofecoSchema\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inferSchema\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"profeco_final_bash.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark/spark-2.3.2-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-2.3.2-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-2.3.2-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Path does not exist: file:/home/luis/Documents/Maestría en ciencia de datos/Métodos de gran escala/Proyecto2/liliproyecto2/profeco_final_bash.csv;'"
     ]
    }
   ],
   "source": [
    "profecoDf = spark.read.format(\"csv\")\\\n",
    "        .option(\"delimiter\", \"|\")\\\n",
    "        .option(\"header\",\"true\")\\\n",
    "        .schema(profecoSchema) \\\n",
    "        .option(\"inferSchema\", \"true\")\\\n",
    "        .load(\"profeco_final_bash.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "profecoDF = spark.read.format('parquet')\\\n",
    "        .load(\"profecoFinal.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "profecoDF = profecoDF.repartition(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- producto: string (nullable = true)\n",
      " |-- presentacion: string (nullable = true)\n",
      " |-- marca: string (nullable = true)\n",
      " |-- categoria: string (nullable = true)\n",
      " |-- catalogo: string (nullable = true)\n",
      " |-- precio: double (nullable = true)\n",
      " |-- fecharegistro: timestamp (nullable = true)\n",
      " |-- cadenacomercial: string (nullable = true)\n",
      " |-- giro: string (nullable = true)\n",
      " |-- nombrecomercial: string (nullable = true)\n",
      " |-- direccion: string (nullable = true)\n",
      " |-- estado: string (nullable = true)\n",
      " |-- municipio: string (nullable = true)\n",
      " |-- latitud: double (nullable = true)\n",
      " |-- longitud: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profecoDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Manipulación, transformación y selección de variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección se utiliza un select para filtrar/seleccionar las varaibles que cumplen con los criterios de la población objetivo: 1) Medicamentos, 2)Ciudad de México (Distrito Federal), 3) Abril y 4) 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlTrans = SQLTransformer( \\\n",
    "    statement=\"SELECT * FROM  __THIS__ \\\n",
    "          WHERE categoria LIKE '%medicamentos%' \\\n",
    "          AND estado='distrito federal' \\\n",
    "          AND MONTH(fecharegistro)=4 \\\n",
    "          AND YEAR(fecharegistro)=2016\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación nos quedamos con las variables relevantes para correr los modelos: producto, marca, cadena comercial, municipio, precio (promedio) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante mencionar que decidimos utilizar el promedio de los precios, esto con el objetivo de modelar la \"tendencia de centralidad\" de los precios; además, nos percatamos que los precios varían poco en el intevalo de tiempo (abril de 2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "formula = SQLTransformer( \\\n",
    "#    statement=\"SELECT producto, marca, precio AS label ,cadenacomercial, municipio FROM  __THIS__ GROUP BY producto\")\n",
    "    statement=\"SELECT producto,marca,cadenacomercial, municipio, MEAN(precio) AS label FROM  __THIS__ GROUP BY producto,marca,cadenacomercial,municipio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionalmente, se declararon algunas variables como categóricas (no strings) para poder correr los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "productoIndexer = StringIndexer(inputCol=\"producto\", outputCol=\"productoIndex\")\n",
    "marcaIndexer = StringIndexer(inputCol=\"marca\", outputCol=\"marcaIndex\")\n",
    "cadenacomercialIndexer = StringIndexer(inputCol=\"cadenacomercial\", outputCol=\"cadenacomercialIndex\")\n",
    "municipioIndexer = StringIndexer(inputCol=\"municipio\", outputCol=\"municipioIndex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utilizó la función VectorAssembler para combinar la lista columnas en una columna de un solo vector. Esto es útil para combinar las características brutas y las características generadas por diferentes transformadores de características en un único vector de características. Este es el formato que se utiliza para correr los modelos de arboles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"productoIndex\", \"marcaIndex\",\"cadenacomercialIndex\",\"municipioIndex\"],\n",
    "    outputCol=\"Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Creación del modelo de RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección se declara el modelo RandomFores y también se declara el pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train a random forest model\n",
    "rf = RandomForestRegressor(labelCol='label',featuresCol='Features',maxBins=400)\n",
    "pipeline = Pipeline(stages=[sqlTrans,formula,productoIndexer,marcaIndexer,cadenacomercialIndexer,municipioIndexer,assembler,rf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  5. Creación del grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección se declaran las características del grid.\n",
    "\n",
    "Los hiperparámetros fueron:\n",
    "\n",
    "1) numTrees: para los valores 64, 96 y 128. En este [_paper_ de 2012](https://www.researchgate.net/publication/230766603_How_Many_Trees_in_a_Random_Forest) de Mayumi, Santoro y Baranauskas se sugiere probar este parámetro en el intervalo [61, 128].\n",
    "\n",
    "2) maxDepth: para los valores 2, 5 y 7. 2 es la profundidad mínima, y como contamos con seis variables explicativas, una profundidad mázima de 7 nos pareció razonable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(rf.maxDepth, [2,5,7])\\\n",
    "    .addGrid(rf.numTrees, [64,96,128])\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  6. Generación de grupos de entrenamiento y validación "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección se declaran los grupos de entrenamiento (80%) y validación (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tvs = TrainValidationSplit(estimator=pipeline,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=RegressionEvaluator(),\n",
    "                           # 80% of the data will be used for training, 20% for validation.\n",
    "                           trainRatio=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  7. Configuración del modelo y selección de parámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí se elige el mejor conjunto de parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = tvs.fit(profecoDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Param(parent='TrainValidationSplitModel_fad83aff1cf4', name='estimator', doc='estimator to be cross-validated')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TrainValidationSplitModel_fad83aff1cf4'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_a306ee1e9b04"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"estimator: estimator to be cross-validated (current: Pipeline_324d9862c7e8)\\nestimatorParamMaps: estimator param maps (current: [{Param(parent='RandomForestRegressor_3b1ecac09ed7', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 2, Param(parent='RandomForestRegressor_3b1ecac09ed7', name='numTrees', doc='Number of trees to train (>= 1).'): 64}, {Param(parent='RandomForestRegressor_3b1ecac09ed7', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 2, Param(parent='RandomForestRegressor_3b1ecac09ed7', name='numTrees', doc='Number of trees to train (>= 1).'): 96}, {Param(parent='RandomForestRegressor_3b1ecac09ed7', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 2, Param(parent='RandomForestRegressor_3b1ecac09ed7', name='numTrees', doc='Number of trees to train (>= 1).'): 128}, {Param(parent='RandomForestRegressor_3b1ecac09ed7', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5, Param(parent='RandomForestRegressor_3b1ecac09ed7', name='numTrees', doc='Number of trees to train (>= 1).'): 64}, {Param(parent='RandomForestRegressor_3b1ecac09ed7', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5, Param(parent='RandomForestRegressor_3b1ecac09ed7', name='numTrees', doc='Number of trees to train (>= 1).'): 96}, {Param(parent='RandomForestRegressor_3b1ecac09ed7', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5, Param(parent='RandomForestRegressor_3b1ecac09ed7', name='numTrees', doc='Number of trees to train (>= 1).'): 128}, {Param(parent='RandomForestRegressor_3b1ecac09ed7', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 7, Param(parent='RandomForestRegressor_3b1ecac09ed7', name='numTrees', doc='Number of trees to train (>= 1).'): 64}, {Param(parent='RandomForestRegressor_3b1ecac09ed7', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 7, Param(parent='RandomForestRegressor_3b1ecac09ed7', name='numTrees', doc='Number of trees to train (>= 1).'): 96}, {Param(parent='RandomForestRegressor_3b1ecac09ed7', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 7, Param(parent='RandomForestRegressor_3b1ecac09ed7', name='numTrees', doc='Number of trees to train (>= 1).'): 128}])\\nevaluator: evaluator used to select hyper-parameters that maximize the validator metric (current: RegressionEvaluator_76a50b6a389c)\\nseed: random seed. (default: 2357993160371581517)\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.explainParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='TrainValidationSplitModel_fad83aff1cf4', name='seed', doc='random seed.'): 2357993160371581517,\n",
       " Param(parent='TrainValidationSplitModel_fad83aff1cf4', name='estimator', doc='estimator to be cross-validated'): Pipeline_324d9862c7e8,\n",
       " Param(parent='TrainValidationSplitModel_fad83aff1cf4', name='estimatorParamMaps', doc='estimator param maps'): [{Param(parent='RandomForestRegressor_3b1ecac09ed7', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 2,\n",
       "   Param(parent='RandomForestRegressor_3b1ecac09ed7', name='numTrees', doc='Number of trees to train (>= 1).'): 64},\n",
       "  {Param(parent='RandomForestRegressor_3b1ecac09ed7', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 2,\n",
       "   Param(parent='RandomForestRegressor_3b1ecac09ed7', name='numTrees', doc='Number of trees to train (>= 1).'): 96},\n",
       "  {Param(parent='RandomForestRegressor_3b1ecac09ed7', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 2,\n",
       "   Param(parent='RandomForestRegressor_3b1ecac09ed7', name='numTrees', doc='Number of trees to train (>= 1).'): 128},\n",
       "  {Param(parent='RandomForestRegressor_3b1ecac09ed7', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5,\n",
       "   Param(parent='RandomForestRegressor_3b1ecac09ed7', name='numTrees', doc='Number of trees to train (>= 1).'): 64},\n",
       "  {Param(parent='RandomForestRegressor_3b1ecac09ed7', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5,\n",
       "   Param(parent='RandomForestRegressor_3b1ecac09ed7', name='numTrees', doc='Number of trees to train (>= 1).'): 96},\n",
       "  {Param(parent='RandomForestRegressor_3b1ecac09ed7', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5,\n",
       "   Param(parent='RandomForestRegressor_3b1ecac09ed7', name='numTrees', doc='Number of trees to train (>= 1).'): 128},\n",
       "  {Param(parent='RandomForestRegressor_3b1ecac09ed7', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 7,\n",
       "   Param(parent='RandomForestRegressor_3b1ecac09ed7', name='numTrees', doc='Number of trees to train (>= 1).'): 64},\n",
       "  {Param(parent='RandomForestRegressor_3b1ecac09ed7', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 7,\n",
       "   Param(parent='RandomForestRegressor_3b1ecac09ed7', name='numTrees', doc='Number of trees to train (>= 1).'): 96},\n",
       "  {Param(parent='RandomForestRegressor_3b1ecac09ed7', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 7,\n",
       "   Param(parent='RandomForestRegressor_3b1ecac09ed7', name='numTrees', doc='Number of trees to train (>= 1).'): 128}],\n",
       " Param(parent='TrainValidationSplitModel_fad83aff1cf4', name='evaluator', doc='evaluator used to select hyper-parameters that maximize the validator metric'): RegressionEvaluator_76a50b6a389c}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_a306ee1e9b04"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{Param(parent='RandomForestRegressor_3b1ecac09ed7', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 2,\n",
       "  Param(parent='RandomForestRegressor_3b1ecac09ed7', name='numTrees', doc='Number of trees to train (>= 1).'): 64},\n",
       " {Param(parent='RandomForestRegressor_3b1ecac09ed7', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 2,\n",
       "  Param(parent='RandomForestRegressor_3b1ecac09ed7', name='numTrees', doc='Number of trees to train (>= 1).'): 96},\n",
       " {Param(parent='RandomForestRegressor_3b1ecac09ed7', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 2,\n",
       "  Param(parent='RandomForestRegressor_3b1ecac09ed7', name='numTrees', doc='Number of trees to train (>= 1).'): 128},\n",
       " {Param(parent='RandomForestRegressor_3b1ecac09ed7', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5,\n",
       "  Param(parent='RandomForestRegressor_3b1ecac09ed7', name='numTrees', doc='Number of trees to train (>= 1).'): 64},\n",
       " {Param(parent='RandomForestRegressor_3b1ecac09ed7', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5,\n",
       "  Param(parent='RandomForestRegressor_3b1ecac09ed7', name='numTrees', doc='Number of trees to train (>= 1).'): 96},\n",
       " {Param(parent='RandomForestRegressor_3b1ecac09ed7', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5,\n",
       "  Param(parent='RandomForestRegressor_3b1ecac09ed7', name='numTrees', doc='Number of trees to train (>= 1).'): 128},\n",
       " {Param(parent='RandomForestRegressor_3b1ecac09ed7', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 7,\n",
       "  Param(parent='RandomForestRegressor_3b1ecac09ed7', name='numTrees', doc='Number of trees to train (>= 1).'): 64},\n",
       " {Param(parent='RandomForestRegressor_3b1ecac09ed7', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 7,\n",
       "  Param(parent='RandomForestRegressor_3b1ecac09ed7', name='numTrees', doc='Number of trees to train (>= 1).'): 96},\n",
       " {Param(parent='RandomForestRegressor_3b1ecac09ed7', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 7,\n",
       "  Param(parent='RandomForestRegressor_3b1ecac09ed7', name='numTrees', doc='Number of trees to train (>= 1).'): 128}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.getEstimatorParamMaps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Métricas de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[217.71386505318395,\n",
       " 223.64828110837172,\n",
       " 218.77543591397645,\n",
       " 128.41655622525496,\n",
       " 130.76820536261008,\n",
       " 127.2947353609827,\n",
       " 110.25212820520983,\n",
       " 108.98707655254567,\n",
       " 108.32031640575397]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.validationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_a306ee1e9b04"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Param(parent='TrainValidationSplitModel_fad83aff1cf4', name='estimator', doc='estimator to be cross-validated')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.getParam('estimator')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  9. Estimación de predicciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente tabla se puede apreciar la estimación de la predicción para los primeros 20 elementos de la tabla.\n",
    "Se puede ver que las variables \"label\" y \"predicción\" tienen valores similares (no están tan lejos entre ellas). Esto quiere decir que las métricas para seleccionar el modelo fueron las adecuadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = model.transform(profecoDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>producto</th>\n",
       "      <th>Features</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>elequine</td>\n",
       "      <td>[28.0, 0.0, 1.0, 3.0]</td>\n",
       "      <td>560.2500</td>\n",
       "      <td>606.711648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>micardis plus</td>\n",
       "      <td>[25.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>643.0000</td>\n",
       "      <td>694.143160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>refresh tears</td>\n",
       "      <td>[210.0, 0.0, 1.0, 7.0]</td>\n",
       "      <td>483.0000</td>\n",
       "      <td>435.823824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>orlistat</td>\n",
       "      <td>[270.0, 0.0, 0.0, 5.0]</td>\n",
       "      <td>276.5000</td>\n",
       "      <td>299.688105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>januvia</td>\n",
       "      <td>[26.0, 0.0, 11.0, 6.0]</td>\n",
       "      <td>867.7250</td>\n",
       "      <td>830.942971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>novotiral</td>\n",
       "      <td>[113.0, 0.0, 5.0, 3.0]</td>\n",
       "      <td>262.9500</td>\n",
       "      <td>291.303895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>neo melubrina</td>\n",
       "      <td>[167.0, 0.0, 3.0, 3.0]</td>\n",
       "      <td>56.9375</td>\n",
       "      <td>69.065521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>zintrepid</td>\n",
       "      <td>[92.0, 0.0, 4.0, 6.0]</td>\n",
       "      <td>1004.6600</td>\n",
       "      <td>981.372142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>diovan</td>\n",
       "      <td>[67.0, 0.0, 14.0, 10.0]</td>\n",
       "      <td>764.9700</td>\n",
       "      <td>744.956637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fluconazol</td>\n",
       "      <td>(257.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>47.0000</td>\n",
       "      <td>61.468471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pulmicort</td>\n",
       "      <td>[170.0, 0.0, 2.0, 5.0]</td>\n",
       "      <td>300.0000</td>\n",
       "      <td>299.143349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>piascledine 300</td>\n",
       "      <td>[79.0, 0.0, 15.0, 0.0]</td>\n",
       "      <td>329.1000</td>\n",
       "      <td>343.353336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>combivent. respimat</td>\n",
       "      <td>[131.0, 0.0, 5.0, 2.0]</td>\n",
       "      <td>594.0500</td>\n",
       "      <td>637.726356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>glibenclamida</td>\n",
       "      <td>[206.0, 0.0, 0.0, 10.0]</td>\n",
       "      <td>15.0000</td>\n",
       "      <td>55.425111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>aspirina protect</td>\n",
       "      <td>[66.0, 0.0, 1.0, 12.0]</td>\n",
       "      <td>53.0000</td>\n",
       "      <td>74.510999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>biomics</td>\n",
       "      <td>[247.0, 0.0, 13.0, 0.0]</td>\n",
       "      <td>591.0000</td>\n",
       "      <td>645.225797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>krytan tek ofteno</td>\n",
       "      <td>[149.0, 0.0, 3.0, 8.0]</td>\n",
       "      <td>650.5000</td>\n",
       "      <td>682.756164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sermion</td>\n",
       "      <td>[84.0, 0.0, 1.0, 4.0]</td>\n",
       "      <td>659.0000</td>\n",
       "      <td>689.928464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>humalog mix 25</td>\n",
       "      <td>[198.0, 0.0, 6.0, 2.0]</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>450.252298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>keppra</td>\n",
       "      <td>[205.0, 0.0, 11.0, 6.0]</td>\n",
       "      <td>1981.9500</td>\n",
       "      <td>1732.697642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               producto                 Features      label   prediction\n",
       "0              elequine    [28.0, 0.0, 1.0, 3.0]   560.2500   606.711648\n",
       "1         micardis plus    [25.0, 0.0, 0.0, 1.0]   643.0000   694.143160\n",
       "2         refresh tears   [210.0, 0.0, 1.0, 7.0]   483.0000   435.823824\n",
       "3              orlistat   [270.0, 0.0, 0.0, 5.0]   276.5000   299.688105\n",
       "4               januvia   [26.0, 0.0, 11.0, 6.0]   867.7250   830.942971\n",
       "5             novotiral   [113.0, 0.0, 5.0, 3.0]   262.9500   291.303895\n",
       "6         neo melubrina   [167.0, 0.0, 3.0, 3.0]    56.9375    69.065521\n",
       "7             zintrepid    [92.0, 0.0, 4.0, 6.0]  1004.6600   981.372142\n",
       "8                diovan  [67.0, 0.0, 14.0, 10.0]   764.9700   744.956637\n",
       "9            fluconazol   (257.0, 0.0, 0.0, 0.0)    47.0000    61.468471\n",
       "10            pulmicort   [170.0, 0.0, 2.0, 5.0]   300.0000   299.143349\n",
       "11      piascledine 300   [79.0, 0.0, 15.0, 0.0]   329.1000   343.353336\n",
       "12  combivent. respimat   [131.0, 0.0, 5.0, 2.0]   594.0500   637.726356\n",
       "13        glibenclamida  [206.0, 0.0, 0.0, 10.0]    15.0000    55.425111\n",
       "14     aspirina protect   [66.0, 0.0, 1.0, 12.0]    53.0000    74.510999\n",
       "15              biomics  [247.0, 0.0, 13.0, 0.0]   591.0000   645.225797\n",
       "16    krytan tek ofteno   [149.0, 0.0, 3.0, 8.0]   650.5000   682.756164\n",
       "17              sermion    [84.0, 0.0, 1.0, 4.0]   659.0000   689.928464\n",
       "18       humalog mix 25   [198.0, 0.0, 6.0, 2.0]   500.0000   450.252298\n",
       "19               keppra  [205.0, 0.0, 11.0, 6.0]  1981.9500  1732.697642"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.select('producto','Features','label','prediction').limit(20).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
